预习算法分析与设计的内容，
继续看论文《Local Model Poisoning Attacks to Byzantine-Robust Federated Learning》，论文的投毒攻击与常见的样本投毒攻击不一样，
一般的投毒攻击是在训练开始前就加入一些人为制造的样本，而本文的是不改变训练样本，改变的是模型参数，在每一次迭代过程中，通过操纵被攻击客户端的模型参数，使得
在全局模型的误差不断增大，最后出现很大误差，模型可用性变得很差。
