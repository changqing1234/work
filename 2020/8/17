看了一篇论文SecureNN: Efficient and Private Neural Network Training，论文是使用秘密共享来保护隐私，
客户端将数据秘密拆分后发个多个服务器，多服务器各自训练自己的模型，最后聚合，模型。这个论文跟以前秘密共享的区别就是使用多个服务器，不限于2个。
所以我今天在协作学习中设置两个服务器，每个客户端都在本地训练模型，然后拆分成两个子模型分别发给两个服务器。两个服务器进行统计，
然后将统计信息发给对方，可以解决师兄上次组会说的客户端掉线问题，同时能防御宋孟凯的服务器端的模型攻击。
还在做实验
