今天将神经网络部分看完了，主要还是理解BP算法，BP算法基于梯度下降策略来进行参数调优，不过使用梯度下降，容易陷入局部极小，
却不是全局最小，所以经常采用随机梯度下降的方法来避免陷入局部极小。然后看了支持向量机的内容，支持向量机基于训练集在样本空间寻找一个划分超平面，将不同的类别区别开来。前三小节的内容容易看懂，主要利用拉格朗日乘子法找出最优的参数
法向量和位移项。后面的软间隔与正则化和支持向量回归感觉看的吃力，主要还是推理公式看的不明白。明天准备再将后两节的内容仔细看一遍，
争取看懂。

