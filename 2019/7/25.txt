今天看论文，学习python。
今天看了预测阶段的安全威胁，在预测阶段，根据攻击者是否知道目标模型的内部结构，可以分为黑盒和白盒攻击。获取目标模型虽然
不容易实现，但是一旦成功，就将对机器学习造成极大的威胁，攻击者会分析目标模型，制造对抗样本攻击，这的攻击具有针对性，很容易
导致分类器做出错误的分类。常用的生成对抗样本方法L-BFGS 、Deepfool，FGSM  、JSMA 。L-BFGS是找到最小扰动，操作最小扰动的操作都能欺骗分类器，
使之做出错误分类。Deepfool方法是找到正常样本的决策边界，跨越边界找到最小扰动。其他的方法理解的不太清楚。明天继续看。