今天找了一篇2019的谷歌发表的联邦学习的论文<TOWARDS FEDERATED LEARNING AT SCALE: SYSTEM DESIGN>
Federated Learning 的特点主要有三个： 训练数据更为真实；2 训练过程中，无需将敏感数据集中到数据中心； 
3.对于监督学习，可以很自然的通过用户交互获取数据的标签。
论文提出了联邦学习的框架，在任务开始时，服务器筛首先选出所有设备端的一个有效子集作为本轮联邦学习任务的执行者，
继而服务器向所有子集中的设备发送数据，数据主要包括TF的计算图以及执行该计算图的方法。
而在每轮训练任务中，服务端于本轮开始时需要向设备端发送当前模型的超参数以及从checkpoint得到的必要状态数据。
之后每个接收到任务的设备根据全局参数和状态数据以及本地数据集执行计算任务，并将更新发送到服务端，最后服务端合并所有设备更新(即执行AvgFed算法)。
提出了服务器和终端的架构
